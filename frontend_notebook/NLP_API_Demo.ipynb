{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specialized-interest",
   "metadata": {},
   "source": [
    "# Google Cloud Natural Language API Demo\n",
    "\n",
    "## Introduction to Jupyter Notebook\n",
    "Jupyter Notebooks are a staple in any data scientist's toolkit. It is a free, open source, interactive data science environment that can function as both an IDE and a visualisation tool. A Jupyter Notebook is a single document where you can run code, display the output and add equations and explainations. Each notebook is a `.ipynb` file, which is a text file that describes the content of the notebook in JSON format.\n",
    "\n",
    "Each Jupter Notebook contains a kernal that can be thought of as a \"computational engine\" that executes the code within the notebook. Notebooks are made up of a number of cells. For example, this piece of text you are reading resides in the first cell of this notebook. They can be markdown cells that display text in-place or code cells. When a code cell is run, the output is displayed below the cell. The order in which cells are run matters! Cells containing functions or variables have to be run before those same functions or variables can be called from a subsequent cell. \n",
    "\n",
    "How to use a Jupyter Notebook:\n",
    "- To run a cell, either click the arrow to the left of the cell or press `ctrl + Enter` after selecting the cell. When a cell is run, a number will appear in square brackets (e.g. [1]) telling you the order in which each cell is run.\n",
    "- To interrupt a cell while it is running, press the button with the black square in the toolbar at the top\n",
    "- To restart the kernal, right-click `kernel` and choose from the list of restart options available\n",
    "\n",
    "\n",
    "## Introduction to NLP API\n",
    "\n",
    "The Natural Language API has several methods for performing analysis and annotation on your text. Each level of analysis provides valuable information for language understanding. These methods are listed below:\n",
    "\n",
    "**Sentiment analysis** inspects the given text and identifies the prevailing emotional opinion within the text, especially to determine a writer's attitude as positive, negative, or neutral. This method returns the sentiment of the text as a whole as well as the sentiment of individual sentences within it. Sentiment analysis is performed through the analyzeSentiment method.\n",
    "\n",
    "**Entity analysis** inspects the given text for known entities (Proper nouns such as public figures, landmarks, and so on. Common nouns such as restaurant, stadium, and so on.) and returns information about those entities. This includes a Wikipedia link (if applicable), the entity type and the salience (a measure of the relevance of the entity to the entire text). Entity analysis is performed with the analyzeEntities method.\n",
    "\n",
    "**Entity sentiment analysis** inspects the given text for known entities (proper nouns and common nouns), returns information about those entities, and identifies the prevailing emotional opinion of the entity within the text, especially to determine a writer's attitude toward the entity as positive, negative, or neutral. An example of how this might be used is when presented with a sentence that contains a number of different emotions; for example, \"I liked the food but the service was terrible\". Entity analysis is performed with the analyzeEntitySentiment method.\n",
    "\n",
    "**Syntactic analysis** extracts linguistic information, breaking up the given text into a series of sentences and tokens (generally, word boundaries), providing further analysis on those tokens. For each word in the text, the API tells you the word's part of speech (noun, verb, adjective, etc.) and how it relates to other words in the sentence. Syntactic Analysis is performed with the analyzeSyntax method.\n",
    "\n",
    "**Content classification** analyzes text content and returns a content category for the content. Content classification is performed by using the classifyText method.\n",
    "\n",
    "Each API call also detects and returns the language, if a language is not specified by the caller in the initial request. A full list of supported languages can be found here: https://cloud.google.com/natural-language/docs/languages\n",
    "\n",
    "Additionally, if you wish to perform several natural language operations on given text using only one API call, the annotateText request can also be used to perform sentiment analysis and entity analysis.\n",
    "\n",
    "#### Further Documentation:\n",
    "https://cloud.google.com/natural-language/docs\n",
    "https://cloud.google.com/natural-language/docs/basics\n",
    "https://cloud.google.com/natural-language/docs/how-to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-ukraine",
   "metadata": {},
   "source": [
    "## The Natural Language API: Set Up And Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-accounting",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "Ensure you have enabled billing, the cloud natural language APIs, and have a service account before running this notebook. \n",
    "\n",
    "You may also need to restart your kernel ('Kernel' in the menu). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user --upgrade google-cloud-language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import google-cloud-language\n",
    "# Make sure that you have installed or upgraded to the latest google-cloud-language using pip\n",
    "from google.cloud import language_v1 as language\n",
    "import pandas as pd\n",
    "#Print all columns and all rows in a panda dataframe\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-logan",
   "metadata": {},
   "source": [
    "#### Set up functions to call Google Natural Language API\n",
    "Here are some examples of the API in action <br>\n",
    "Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Google at https://codelabs.developers.google.com/codelabs/cloud-natural-language-python3#7\n",
    "# Probably would be better off changing all the functions to follow the Google standard ones from the codelab, and then making \n",
    "# small modifications to the rest of the code to make it all work together.\n",
    "\n",
    "def analyze_text_sentiment(text):\n",
    "    client = language.LanguageServiceClient()\n",
    "    document = language.Document(content=text, type_=language.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    response = client.analyze_sentiment(document=document)\n",
    "\n",
    "    sentiment = response.document_sentiment\n",
    "    results = dict(\n",
    "        text=text,\n",
    "        score=f\"{sentiment.score:.1%}\",\n",
    "        magnitude=f\"{sentiment.magnitude:.1%}\",\n",
    "    )\n",
    "    \n",
    "    # Get sentiment for all sentences in the document\n",
    "    sentence_sentiment = []\n",
    "    for sentence in response.sentences:\n",
    "        item={}\n",
    "        item[\"text\"]=sentence.text.content\n",
    "        item[\"sentiment score\"]=sentence.sentiment.score\n",
    "        item[\"sentiment magnitude\"]=sentence.sentiment.magnitude\n",
    "        sentence_sentiment.append(item)\n",
    "    \n",
    "    return sentence_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Stocks are going down on the NASDAQ\"\n",
    "analyze_text_sentiment(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-powder",
   "metadata": {},
   "source": [
    "Syntactic Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntax Analysis\n",
    "def gcp_analyze_syntax(text, debug=0):\n",
    "    \"\"\"\n",
    "    Analyzing Syntax in a String\n",
    "\n",
    "    Args:\n",
    "      text The text content to analyze\n",
    "    \"\"\"\n",
    "\n",
    "    client = language.LanguageServiceClient()\n",
    "    document = language.Document(content=text, type_=language.Document.Type.PLAIN_TEXT)\n",
    "    response = client.analyze_syntax(document=document)\n",
    "    \n",
    "    output = []   \n",
    "    # Loop through tokens returned from the API\n",
    "    for token in response.tokens:\n",
    "        word = {}\n",
    "        # Get the text content of this token. Usually a word or punctuation.\n",
    "        text = token.text  \n",
    "\n",
    "        # Get the part of speech information for this token.\n",
    "        # Parts of spech are as defined in:\n",
    "        # http://www.lrec-conf.org/proceedings/lrec2012/pdf/274_Paper.pdf\n",
    "        part_of_speech = token.part_of_speech\n",
    "        # Get the tag, e.g. NOUN, ADJ for Adjective, et al.\n",
    "        \n",
    "        # Get the dependency tree parse information for this token.\n",
    "        # For more information on dependency labels:\n",
    "        # http://www.aclweb.org/anthology/P13-2017\n",
    "        dependency_edge = token.dependency_edge   \n",
    "        \n",
    "        word[\"word\"]=text.content\n",
    "        word[\"begin_offset\"]=text.begin_offset        \n",
    "        word[\"part_of_speech\"]=language.PartOfSpeech.Tag(part_of_speech.tag).name\n",
    "        \n",
    "        # Get the voice, e.g. ACTIVE or PASSIVE\n",
    "        word[\"Voice\"]=language.PartOfSpeech.Voice(part_of_speech.voice).name\n",
    "        word[\"Tense\"]=language.PartOfSpeech.Tense(part_of_speech.tense).name\n",
    "        \n",
    "        # See API reference for additional Part of Speech information available\n",
    "        # Get the lemma of the token. Wikipedia lemma description\n",
    "        # https://en.wikipedia.org/wiki/Lemma_(morphology)        \n",
    "        word[\"Lemma\"]=token.lemma\n",
    "        word[\"index\"]=dependency_edge.head_token_index\n",
    "        word[\"Label\"]=language.DependencyEdge.Label(dependency_edge.label).name\n",
    "        \n",
    "        if debug:\n",
    "            print(u\"Token text: {}\".format(text.content))\n",
    "            print(\n",
    "                u\"Location of this token in overall document: {}\".format(text.begin_offset)\n",
    "            ) \n",
    "            print(\n",
    "                u\"Part of Speech tag: {}\".format(\n",
    "                    language.PartOfSpeech.Tag(part_of_speech.tag).name\n",
    "                )\n",
    "            )        \n",
    "\n",
    "            print(u\"Voice: {}\".format(language.PartOfSpeech.Voice(part_of_speech.voice).name))\n",
    "            # Get the tense, e.g. PAST, FUTURE, PRESENT, et al.\n",
    "            print(u\"Tense: {}\".format(language.PartOfSpeech.Tense(part_of_speech.tense).name))\n",
    "\n",
    "            print(u\"Lemma: {}\".format(token.lemma))\n",
    "\n",
    "            print(u\"Head token index: {}\".format(dependency_edge.head_token_index))\n",
    "            print(\n",
    "                u\"Label: {}\".format(language.DependencyEdge.Label(dependency_edge.label).name)\n",
    "            )\n",
    "        \n",
    "        output.append(word)\n",
    "        \n",
    "\n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    if debug:\n",
    "        print(u\"Language of the text: {}\".format(response.language))\n",
    "    return (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_analyze_syntax(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-editor",
   "metadata": {},
   "source": [
    "Entity Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity Analysis\n",
    "def gcp_analyze_entities(text, debug=0):\n",
    "    \"\"\"\n",
    "    Analyzing Entities in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "\n",
    "    client = language.LanguageServiceClient()\n",
    "    document = language.Document(content=text, type_=language.Document.Type.PLAIN_TEXT)\n",
    "    response = client.analyze_entities(document=document)\n",
    "    output = []   \n",
    "    \n",
    "    # Loop through entitites returned from the API\n",
    "    for entity in response.entities:\n",
    "        item = {}\n",
    "        item[\"name\"]=entity.name\n",
    "        item[\"type\"]=language.Entity.Type(entity.type_).name\n",
    "        item[\"Salience\"]=entity.salience\n",
    "        \n",
    "        if debug:\n",
    "            print(u\"Representative name for the entity: {}\".format(entity.name))\n",
    "\n",
    "            # Get entity type, e.g. PERSON, LOCATION, ADDRESS, NUMBER, et al\n",
    "            print(u\"Entity type: {}\".format(language.Entity.Type(entity.type_).name))\n",
    "\n",
    "            # Get the salience score associated with the entity in the [0, 1.0] range\n",
    "            print(u\"Salience score: {}\".format(entity.salience))\n",
    "\n",
    "        # Loop over the metadata associated with entity. For many known entities,\n",
    "        # the metadata is a Wikipedia URL (wikipedia_url) and Knowledge Graph MID (mid).\n",
    "        # Some entity types may have additional metadata, e.g. ADDRESS entities\n",
    "        # may have metadata for the address street_name, postal_code, et al.\n",
    "        for metadata_name, metadata_value in entity.metadata.items():\n",
    "            item[metadata_name]=metadata_value\n",
    "            if debug:\n",
    "                print(u\"{}: {}\".format(metadata_name, metadata_value))\n",
    "\n",
    "        # Loop over the mentions of this entity in the input document.\n",
    "        # The API currently supports proper noun mentions.\n",
    "        if debug:\n",
    "            for mention in entity.mentions:\n",
    "                print(u\"Mention text: {}\".format(mention.text.content))\n",
    "                # Get the mention type, e.g. PROPER for proper noun\n",
    "                print(\n",
    "                    u\"Mention type: {}\".format(language.EntityMention.Type(mention.type_).name)\n",
    "                )\n",
    "        output.append(item)\n",
    "    \n",
    "    # Get the language of the text, which will be the same as\n",
    "    # the language specified in the request or, if not specified,\n",
    "    # the automatically-detected language.\n",
    "    if debug:\n",
    "        print(u\"Language of the text: {}\".format(response.language))\n",
    "    \n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_analyze_entities(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-submission",
   "metadata": {},
   "source": [
    "Content Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Classification\n",
    "\n",
    "def gcp_classify_text(text):\n",
    "    client = language.LanguageServiceClient()\n",
    "    document = language.Document(content=text, type_=language.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    response = client.classify_text(document=document)\n",
    "\n",
    "    for category in response.categories:\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"category  : {category.name}\")\n",
    "        print(f\"confidence: {category.confidence:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-pilot",
   "metadata": {},
   "source": [
    "A longer piece of text is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Although most people consider piranhas to be quite dangerous, they are, for the most part, entirely harmless. \\n\\\n",
    "Piranhas rarely feed on large animals; they eat smaller fish and aquatic plants. When confronted with humans, piranhas’ \\n\\\n",
    "first instinct is to flee, not attack. Their fear of humans makes sense. Far more piranhas are eaten by people than people \\n\\\n",
    "are eaten by piranhas. If the fish are well-fed, they won’t bite humans.\"\n",
    "\n",
    "gcp_classify_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-watch",
   "metadata": {},
   "source": [
    "## Demo 1 - Process a single news article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-rally",
   "metadata": {},
   "source": [
    "#### Analyze Syntax\n",
    "Syntactic Analysis breaks up the given text into a series of sentences and tokens and provides linguistic information about those tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_syntax=gcp_analyze_syntax(text)\n",
    "df_syntax = pd.DataFrame(text_syntax)\n",
    "df_syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-homeless",
   "metadata": {},
   "source": [
    "#### Analyze Entities\n",
    "Entity Analysis inspects the given text for known entities (proper nouns such as public figures, landmarks, etc.), and returns information about those entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities=gcp_analyze_entities(text)\n",
    "df_entities = pd.DataFrame(entities)\n",
    "df_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-garbage",
   "metadata": {},
   "source": [
    "#### Classify Documents\n",
    "Google Natual Language API classifies documents into these major categories: <br>\n",
    "Adult\n",
    "\n",
    "Arts & Entertainment\n",
    "\n",
    "Autos & Vehicles\n",
    "\n",
    "Beauty & Fitness\n",
    "\n",
    "Books & Literature\n",
    "\n",
    "Business & Industrial\n",
    "\n",
    "Computers & Electronics\n",
    "\n",
    "Finance\n",
    "\n",
    "Food & Drink\n",
    "\n",
    "Games\n",
    "\n",
    "Health\n",
    "\n",
    "Hobbies & Leisure\n",
    "\n",
    "Home & Garden\n",
    "\n",
    "Internet & Telecom\n",
    "\n",
    "Jobs & Education\n",
    "\n",
    "Law & Government\n",
    "\n",
    "News\n",
    "\n",
    "Online Communities\n",
    "\n",
    "People & Society\n",
    "\n",
    "Pets & Animals\n",
    "\n",
    "Real Estate\n",
    "\n",
    "Reference\n",
    "\n",
    "Science\n",
    "\n",
    "Sensitive Subjects\n",
    "\n",
    "Shopping\n",
    "\n",
    "Sports\n",
    "\n",
    "Travel\n",
    "\n",
    "A full list of categories and subcategories could be found here: https://cloud.google.com/natural-language/docs/categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-nurse",
   "metadata": {},
   "source": [
    "#### Analyze Sentiment\n",
    "Interpreting Google Sentiment Analysis Values:\n",
    "\n",
    "Sentiment Score - a number from -1.0 to 1.0 indicating how positive or negative the statement is.\n",
    "\n",
    "Sentiment Magnitude - a number ranging from 0 to infinity that represents the weight of sentiment expressed in the statement, regardless of being positive or negative. This value is often proportional to the length of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment, magnitude, sentence_sentiment=gcp_analyze_sentiment(text) <- never declared\n",
    "sentence_sentiment = analyze_text_sentiment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment = pd.DataFrame(sentence_sentiment)\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-blocking",
   "metadata": {},
   "source": [
    "## Demo 2 - Process sample news articles from Refinitiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# news_sample=\"github/gcp/FinancialServicesHeadline100.csv\" \n",
    "# news_sample=\"gs://ml-core-shared-standard-bucket/data/FinancialServicesHeadline100.csv\"\n",
    "# df = pd.read_csv(news_sample)\n",
    "df = pd.read_csv('reuters_headlines.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df[\"Headlines\"]\n",
    "print(\"size of document:\", text.shape)\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all news into one document\n",
    "text_all= df[\"Headlines\"].to_string(index=False)\n",
    "#print(text_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-above",
   "metadata": {},
   "source": [
    "#### Analyze Syntax\n",
    "Syntactic Analysis breaks up the given text into a series of sentences and tokens and provides linguistic information about those tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each news as a separate document\n",
    "\n",
    "df_text_syntax=pd.DataFrame()\n",
    "for text in df[\"Headlines\"]:\n",
    "    item=gcp_analyze_syntax(text)\n",
    "    df_text_syntax=df_text_syntax.append(pd.DataFrame(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of output:\", df_text_syntax.shape)\n",
    "df_text_syntax.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-therapist",
   "metadata": {},
   "source": [
    "#### Analyze Entities\n",
    "Entity Analysis inspects the given text for known entities (proper nouns such as public figures, landmarks, etc.), and returns information about those entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each article independently\n",
    "\n",
    "df_entities=pd.DataFrame()\n",
    "for text in df[\"Headlines\"]:\n",
    "    item=pd.DataFrame(gcp_analyze_entities(text))\n",
    "    df_entities=df_entities.append(item, ignore_index=True)\n",
    "# entities=gcp_analyze_entities(text_all)\n",
    "# df_entities2 = pd.DataFrame(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of output:\", df_entities.shape)\n",
    "df_entities.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-underwear",
   "metadata": {},
   "source": [
    "#### Classify Documents\n",
    "Google Natual Language API classifies documents into these major categories: <br>\n",
    "Adult\n",
    "\n",
    "Arts & Entertainment\n",
    "\n",
    "Autos & Vehicles\n",
    "\n",
    "Beauty & Fitness\n",
    "\n",
    "Books & Literature\n",
    "\n",
    "Business & Industrial\n",
    "\n",
    "Computers & Electronics\n",
    "\n",
    "Finance\n",
    "\n",
    "Food & Drink\n",
    "\n",
    "Games\n",
    "\n",
    "Health\n",
    "\n",
    "Hobbies & Leisure\n",
    "\n",
    "Home & Garden\n",
    "\n",
    "Internet & Telecom\n",
    "\n",
    "Jobs & Education\n",
    "\n",
    "Law & Government\n",
    "\n",
    "News\n",
    "\n",
    "Online Communities\n",
    "\n",
    "People & Society\n",
    "\n",
    "Pets & Animals\n",
    "\n",
    "Real Estate\n",
    "\n",
    "Reference\n",
    "\n",
    "Science\n",
    "\n",
    "Sensitive Subjects\n",
    "\n",
    "Shopping\n",
    "\n",
    "Sports\n",
    "\n",
    "Travel\n",
    "\n",
    "A full list of categories and subcategories could be found here:\n",
    "https://cloud.google.com/natural-language/docs/categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall document classification\n",
    "gcp_classify_text(text_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each article independently\n",
    "\n",
    "df_sentiment=pd.DataFrame()\n",
    "item_sentiment=pd.DataFrame(columns=[\"text\", \"sentiment score\",\"sentiment magnitude\"])\n",
    "for text in df[\"Headlines\"]:\n",
    "    sentiment_output = analyze_text_sentiment(text)\n",
    "    item_sentiment.loc[0, \"text\"]=sentiment_output[0].get('text')\n",
    "    item_sentiment.loc[0, \"sentiment score\"]= sentiment_output[0].get('sentiment score')\n",
    "    item_sentiment.loc[0,\"sentiment magnitude\"]= sentiment_output[0].get('sentiment magnitude')\n",
    "    \n",
    "    df_sentiment=df_sentiment.append(item_sentiment, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiment.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sentiment Scores\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.rcParams.update({'figure.figsize':(16,8)})\n",
    "\n",
    "x = df_sentiment[\"sentiment score\"]\n",
    "y =  df_sentiment[\"sentiment magnitude\"]\n",
    "\n",
    "sns.scatterplot(data= df_sentiment[[\"sentiment score\", \"sentiment magnitude\"]])\n",
    "                \n",
    "n_bins=30\n",
    "\n",
    "#plt.hist(x, bins=n_bins)\n",
    "#plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "# We can set the number of bins with the `bins` kwarg\n",
    "axs[0].set_xlabel(\"Sentiment Score\")\n",
    "axs[0].set_ylabel(\"percentage\")\n",
    "axs[0].set_title('Histogram of Sentiment Score')\n",
    "axs[1].set_xlabel(\"Sentiment Magnitude\")\n",
    "axs[1].set_title('Histogram of Sentiment Magnitude')\n",
    "\n",
    "axs[0].hist(x, bins=n_bins)\n",
    "axs[1].hist(y, bins=n_bins)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(tight_layout=True)\n",
    "hist = ax.hist2d(x, y, norm=colors.LogNorm())\n",
    "plt.title(\"Sentiment Score and Magnitude 2-D Distribution\")\n",
    "ax.set_xlabel(\"Sentiment Score\")\n",
    "ax.set_ylabel(\"Sentiment Magnitude\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
